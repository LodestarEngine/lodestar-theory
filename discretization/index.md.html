<meta charset="utf-8" emacsmode="-*- markdown -*-">

_Last updated:_ July 4, 2021.

In real-life we can study the behavior of dynamic processes at
arbitrarily small time scales, but digital systems inherently operate as
sampled data systems. Be it the rate at which we receive sensor
readings, or the rate at which we update our control commands, there is
a finite (often assumed to be fixed) time between events when we use
digital systems that interact with the physical world.

This simple observation has spawned an extensive field of research into
discrete-time systems. Consider the following linear time-invariant
continuous-time system:

\begin{equation}\label{eq:CTLTI}
\dot{x}(t) = A x(t) + B u(t),
\end{equation}

where we have time $t \in \mathbb{R}$, as well as state
$x(t) \in \mathbb{R}^n$ and control input $u(t) \in \mathbb{R}^m$ at
time $t$. $A \in \mathbb{R}^{n \times n}$ is our state matrix, and
$B \in \mathbb{R}^{n \times m}$ is our input matrix. For
$\eqref{eq:CTLTI}$, we need to solve the ordinary differential equation
to obtain the state at a later time. This is intuitive from a physical
perspective, but if we consider that digital systems often hold $u(t)$
constant for a (fixed) period of time before updating, one could ask for
a representation of the following form:

\begin{equation}\label{eq:DTLTI}
x_{k + 1} = \Phi x_k + \Gamma u_k,
\end{equation}

where $\cdot_k$ denotes that variable at _sampling index_
$k \in \mathbb{N}$. In physical time, a single sampling period
corresponds to some predefined $\Delta t > 0$, which is often governed
by our actuation and sensing capabilities. If we can obtain an
expression such as $\eqref{eq:DTLTI}$, we would only need to supply our
state and control input at the start of a sampling period, and we would
simply obtain the next state as a result, simply by summation and
multiplication operations. This completely forgoes the need for
numerical integration, and a system representation of this form often
gives rise to simpler controller and estimator synthesis algorithm, as
well as cleaner implementations on hardware.

In this article, we study how to go from a continuous-time linear
time-invariant system described by $\{A, B\}_c$ to a discrete-time
system described by $\{\Phi, \Gamma, \Delta t\}_d$.

Approaches to Discretization
------------------------------------------------------------------------

We will cover two discretization approaches in this article:
1. Zero-order hold using matrix exponentials,
2. Zero-order hold using the generalized bilinear transform
    (and its specializations).

The first approach is provides and exact solution, but comes at the
expense of greater computational cost; in practice, this approach would
only be considered when the system dimensions are small
(say, less than $\mathcal{O}(10)$).

The second approach is much more performant, but is not exact. This
approach is used to a much greater extent when the system dimensions are
large, or speed is of the essence. 

In some cases it is possible to revert a discrete-time system to a
continuous-time version, but this comes with questions regarding the
uniqueness of the continuous-time system. We refer to this inverse
process as _continuization_, and show that is possible when the systems
that we obtain from this process are unique.

Zero-order Hold Discretization
------------------------------------------------------------------------

Our discussion of zero-order hold (ZOH) discretization largely parallels
that of [#Franklin98] (Sec. 4.3.3, pp. 101--107). We consider a
continuous-time linear time invariant system of the form
$\eqref{eq:CTLTI}$:

\begin{equation*}
\dot{x}(t) = A x(t) + B u(t). \tag{1}
\end{equation*}

### Homogeneous Solution

We start by considering the homogeneous solution to this ordinary
differential equation (ODE), i.e. the solution for the case where
$u := 0$, and $x_h (t_0) = x_0 \in \mathbb{R}^n$:

\begin{equation}\label{eq:homogeneous_polynomial_solution}
x_h (t) = A_0 + A_1 (t - t_0) + A_2 (t - t_0)^2 + \ldots = \sum_{i = 0}^\infty A_i (t - t_0)^i,
\end{equation}

where we assume that $x_h$ is a sufficiently smooth function such that
a solution of the form of $\eqref{eq:homogeneous_polynomial_solution}$
exists[^ADM].

If we then differentiate $\eqref{eq:homogeneous_polynomial_solution}$
with respect to time, we simply obtain

\begin{equation}
\begin{split}
\dot{x_h} (t) &= A x_h (t) \\
&= A_1 + 2 A_2 (t - t_0) + 3 A_3 (t - t_0)^2 + \ldots \\
&= \sum_{i = 1}^\infty i A_i (t - t_0)^{i - 1},
\end{split}\label{eq:homogeneous_polynomial_solution_first_deriv}
\end{equation}

Setting $t = t_0$ then gives $A_1 = A x_0$. Repeated differentiation of
$\eqref{eq:homogeneous_polynomial_solution_first_deriv}$ yields a 
increasingly higher-order Taylor series expansion about $x_0$ and $t_0$,
which gives the following series solution:

\begin{equation}
\begin{split}
x_h (t) &= \left[ I + A (t - t_0) + \frac{A^2}{2} (t - t_0)^2 + \frac{A^3}{6} (t - t_0)^3 + \ldots \right] x_0 \\
&= \sum_{i = 0}^\infty \frac{A^i}{i!} (t - t_0)^i x_0.
\end{split}\label{eq:homogeneous_polynomial_solution_series_expansion}
\end{equation}

Upon closer inspection we find that
$\eqref{eq:homogeneous_polynomial_solution_series_expansion}$ is simply
the Taylor series expansion of the exponential function:

\begin{equation}\label{eq:homogeneous_solution}
x_h (t) = \exp(A (t-t_0)) x(t_0).
\end{equation}

$\eqref{eq:homogeneous_solution}$ implicitly defines matrix
exponentials:

\begin{equation}\label{eq:matrix_exponential}
\exp(X) := \sum_{i = 0}^\infty \frac{X^i}{i!}.
\end{equation}

$\newenvironment{rcases}
{\left.\begin{aligned}}
{\end{aligned}\right\rbrace}$

!!! Tip
    **Matrix Exponentials.**
    Some properties of matrix exponentials as they relate to the
    homogeneous solutions of $\eqref{eq:CTLTI}$ are given here:

    - The solution at any future time can be computed from any solution
        at any other time:

    \begin{equation}
    \begin{split}
    &\begin{rcases} x(t_1) &= \exp(A (t_1 - t_0)) x(t_0) \\ x(t_2) &= \exp(A (t_2 - t_0)) x(t_0) \end{rcases} \\
    &\Rightarrow \\
    &\begin{cases} x(t_2) &= \exp(A (t_2 - t_1)) x(t_1) \\ \exp(A (t_2 - t_0)) &= \exp(A (t_2 - t_1)) \exp(A (t_1 - t_0)) \end{cases}.
    \end{split}\tag{A1}
    \label{eq:matrix_exponential_memoryless}
    \end{equation}

    - A corollary to $\eqref{eq:matrix_exponential_memoryless}$:

    \begin{equation}\label{eq:matrix_exponential_reverse_time}
    \exp(- A (t_1 - t_0)) \exp(A (t_1 - t_0)) = I. \tag{A2}
    \end{equation}

        $\eqref{eq:matrix_exponential_reverse_time}$ implies that the
        reverse time solution can be obtained by simply changing the
        sign of $t$.


### Particular Solution

Since system $\eqref{eq:CTLTI}$ is linear, we will be able to
superimpose the homogeneous solution obtained in
$\eqref{eq:homogeneous_solution}$ and the particular solution that we
will obtain next.

We will employ the method of variation of parameters. We assume that our
particular solution has the following form:

\begin{equation}\label{eq:particular_solution_structure}
x_p (t) = \exp(A (t - t_0)) v(t),
\end{equation}

where $v : [t_0, \infty) \to \mathbb{R}^n$ is to be obtained.

After differentiating $\eqref{eq:particular_solution_structure}$ with
respect to time and substituting into $\eqref{eq:CTLTI}$, we
obtain[^franklinTypo]:

\begin{equation*}
\begin{split}
&A \exp(A (t - t_0)) v(t) + \exp(A (t - t_0)) \dot{v}(t) \\
= &A \exp(A (t - t_0)) v(t) + B u(t),
\end{split}
\end{equation*}

which yields after solving for $\dot{v}(t)$:

\begin{equation*}
\dot{v}(t) = \exp(- A (t - t_0)) B u(t).
\end{equation*}

If we consider the case where $u(t) = 0$ for $t < t_0$, we obtain the
following expression for $v(t)$:

\begin{equation*}
v(t) = \int_{t_0}^t \exp(- A (\tau - t_0)) B u(\tau) \ \text{d}\tau.
\end{equation*}

Substituting this result into
$\eqref{eq:particular_solution_structure}$, we have

\begin{equation}
\begin{split}
x_p (t) &= \exp(A (t - t_0)) \int_{t_0}^t \exp(- A (\tau - t_0)) B u(\tau) \ \text{d}\tau \\
&= \int_{t_0}^t \exp(- A (t - \tau)) B u(\tau) \ \text{d}\tau,
\end{split}\label{eq:particular_solution}
\end{equation}
where we have employed the result from
$\eqref{eq:matrix_exponential_memoryless}$. The solution to system
$\eqref{eq:CTLTI}$ is then given as

\begin{equation}\label{eq:CTLTI_solution}
x(t) = \exp(A (t-t_0)) x(t_0) + \int_{t_0}^t \exp(- A (t - \tau)) B u(\tau) \ \text{d}\tau,
\end{equation}
which is sum of $x_h$ and $x_p$.

### Discretization

We now substitute $t = (k + 1) \Delta t$ and $t_0 = k \Delta t$ into
$\eqref{eq:CTLTI_solution}$. This yields after some simplification

\begin{equation}
\begin{split}
x((k + 1) \Delta t) &= \exp(A \Delta t) x(k \Delta t) \\
&+ \int_{k \Delta t}^{(k + 1) \Delta t} \exp(A ((k+1) \Delta t - \tau)) B u(\tau) \ \text{d}\tau.
\end{split}\label{eq:CTLTI_continuous_update}
\end{equation}

If we _hold_ $u(t)$ constant for $t \in [k \Delta t, (k + 1) \Delta t)$,
we obtain what is known as zero-order[^zohOrigin] hold (ZOH) with no
delay (see Fig. [zohDiagram]).

**************************************************
*        ▲       .--.      .------.              *
* Cont.  ┊      /    |    /        |             *
* Signal ┊     /      '--'         +             *
*        ┊    /                     \            *
*        *---':┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄+--▶ t      *
*        :<-->:                                  *
*        ▲ Δt :               .------.           *
*        ┊    :   .---.       |      |           *
* ZOH    ┊    :   |   |   .---'      |           *
*        ┊    :   |   +---+          |           *
*        ┊    .---'                  |           *
*        *----+┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄+--▶ k      *
*         0   1   2   3   4   5   6   7          *
**************************************************
[Figure [zohDiagram]: Illustration of zero-order hold sampling]

After a change of variables in the integral, we can obtain a
simplified expression of $\eqref{eq:CTLTI_continuous_update}$ for this
particular case:

\begin{equation}\label{eq:CTLTI_continuous_update_ZOH}
x((k + 1) \Delta t) = \exp(A \Delta t) x(k \Delta t) + \int_{0}^{\Delta t} \exp(A \eta) \ \text{d}\eta \ B u(k \Delta t),
\end{equation}

from which we can easily extract
$$\begin{align}
\Phi &:= \exp(A \Delta t), \\
\Gamma &:= \int_{0}^{\Delta t} \exp(A \eta) \ \text{d}\eta \ B.
\end{align}$$

From these expressions we obtain the discrete-time system
$\{\Phi, \Gamma, \Delta t\}_d$ as given in $\eqref{eq:DTLTI}$, with the
assumption that the original continuous-time system $\eqref{eq:CTLTI}$
is subject to instantaneous zero-order hold control.

#### Algorithmic Implementation

While the expressions given for $\Phi$ and $\Gamma$ give us analytical
closure, it is not yet clear how to compute either of them, in
particular $\Gamma$.

We have previously seen that we can expand $\Phi$ as

\begin{equation}\label{eq:Phi_series_expansion}
    \Phi(\Delta t) = \exp(A \Delta t) = \sum_{i = 0}^\infty \frac{A^i \Delta t^i}{i!}.
\end{equation}

The first term in $\Gamma$ is simply the integral of $\Phi(\eta)$ from 0
to $\Delta t$, which can be evaluated as

\begin{equation}\label{eq:Gamma_series_expansion}
    \Gamma = \int_0^{\Delta t} \exp(A \eta) \ \text{d}\eta \ B = \sum_{i = 0}^\infty \frac{A^i \Delta t^{i+1}}{(i+1)!} B.
\end{equation}

To evaluate $\eqref{eq:Gamma_series_expansion}$, it is possible to
leverage the definition of the matrix exponential. From
$\eqref{eq:matrix_exponential}$, it is clear that the matrix exponential
is only defined on square matrices (after the first multiplication, the
dimension would not agree). Since $A \in \mathbb{R}^{n \times n}$ this
checks out for $\Phi$. This leads to the question of how to
compute $\Gamma$, given that $\eqref{eq:Gamma_series_expansion}$ closely
resembles $\eqref{eq:Phi_series_expansion}$.

We construct the following block matrix:

\begin{equation}\label{eq:ZOH_exponential_matrix}
    E := \begin{bmatrix}
        A \Delta t & B \Delta t \\
        0_{m \times n} & 0_{m \times m}
    \end{bmatrix} \in \mathbb{R}^{(n+m) \times (n+m)}
\end{equation}

Let's consider the expansion of $E^e := \exp(E \Delta t)$:

\begin{equation}\label{eq:ZOH_exponential_matrix_expansion}
    E^e = I + \begin{bmatrix}
        A \Delta t & B \Delta t \\
        0 & 0
    \end{bmatrix}
    + \begin{bmatrix}
    A^2 \Delta t^2 / 2 & A B \Delta t^2 / 2 \\
    0 & 0
    \end{bmatrix}
    + \begin{bmatrix}
    A^3 \Delta t^3 / 6 & A^2 B \Delta t^3 / 6 \\
    0 & 0
    \end{bmatrix}
    <!-- + \begin{bmatrix}
    A^4 \Delta t^4 / 24 & A^3 B \Delta t^4 / 24 \\
    0 & 0
    \end{bmatrix} -->
    + \ldots.
\end{equation}

A closer look at $\eqref{eq:ZOH_exponential_matrix_expansion}$--in
particular the top left and top right blocks of the matrix--reveals
some interesting properties:

$$\begin{align*}
    E_{11}^e &= I + A \Delta t + \frac{A^2 \Delta t^2}{2} + \frac{A^3 \Delta t^3}{6} + \frac{A^4 \Delta t^4}{24} + \ldots, \\
    E_{12}^e &= 0 + B \Delta t + \frac{A B \Delta t^2}{2} + \frac{A^2 B \Delta t^3}{6} + \frac{A^3 B \Delta t^4}{24} + \ldots.
\end{align*}$$

We can see that with this choice of $E$, we obtain $\Phi = E_{11}^e$
and $\Gamma = E_{12}^e$! In other words, we have

\begin{equation*}
E^e = \begin{bmatrix}
    \Phi & \Gamma \\
    0 & I
\end{bmatrix}.
\end{equation*}

### Continuization

To go from a discrete-time system $\{\Phi, \Gamma, \Delta t\}_d$ to a
continuous time system $\{A, B\}_c$, we will need some new analytical
tools. Simply going off intuition, one would think that some information
gets lost when we go from a continuous-time system to a discrete-time
system, meaning that an arbitrary discrete-time system need not be
continuizable in a unique sense.

Consider the definition of $E$ given in
Sec. [Algorithmic Implementation]. As shown before, we find that the
matrix exponential has the following effect on $E$:

\begin{equation*}
E = \begin{bmatrix}
A \Delta t & B \Delta t \\
0 & 0
\end{bmatrix}
\xrightarrow[]{\exp}
E^e = \begin{bmatrix}
\Phi & \Gamma \\
0 & I
\end{bmatrix}.
\end{equation*}

In the case of scalars, the inverse operation to the exponential
function would taking the natural logarithm. A similar notion exists for
matrices, and is known as the _matrix logarithm_. If the matrix 
logarithm is defined for some square matrix $X$, this would imply the
existence of some $X^l := \log(X)$, such that $\exp(X^l) = X$.

Let's assume the matrix logarithm of $E^e$ exists. This would mean that
there is some $(E^e)^l$ such that $\exp((E^e)^l) = E^e$. If $(E^e)^l$ is
unique, this would mean it is equal to $E$, since
$\exp(E) = \exp((E^e)^l) = E^e$. This gives us the following relation:

\begin{equation*}
E^e = \begin{bmatrix}
\Phi & \Gamma \\
0 & I
\end{bmatrix}
\xrightarrow[]{\log}
E = \begin{bmatrix}
A \Delta t & B \Delta t \\
0 & 0
\end{bmatrix}
\xrightarrow[]{\div \Delta t}
\begin{bmatrix}
A & B \\
0 & 0
\end{bmatrix},
\end{equation*}
from which we can obtain $\{A, B\}_c$.

!!! Tip
    **Uniqueness of the Matrix Logarithm**
    As discussed in [#Al-Mohy12], for any nonsingular matrix there are
    infinitely many possible matrix logarithms that one could define.

    However, for a matrix $X$ with _no_ eigenvalues on
    $\mathbb{R}^- := (-\infty, 0]$, there exists a unique logarithm of
    $X$ whose eigenvalues have their imaginary part in the interval
    $(-\pi, \pi)$ ([#Higham08], Thm. 1.31, p. 20). This unique logarithm
    is called the _principle logarithm_ of $X$.

    Let's consider the eigenvalues of $E^e$:
    \begin{equation*}
    \det(E^e - \lambda I) = \det \begin{bmatrix}
    \Phi - \lambda I & \Gamma \\
    0 & (1 - \lambda) I
    \end{bmatrix} = \det(\Phi - \lambda I) \det((1 - \lambda) I),
    \end{equation*}
    where we used a well known identity involving the determinant of
    block matrices.
    This clearly shows that the eigenvalues are 1 and the eigenvalues
    of $\Phi$. A necessary condition for stability reads that $\Phi$
    must have its eigenvalues inside the unit disk, but there are no
    provisions that say that the eigenvalues must not be on the negative
    real axis. This condition should therefore be checked manually to
    ensure the existence of a unique matrix logarithm of $E^e$.
* * *

[^ADM] This solution can be expressed by Adomian polynomials using the
[Adomian decomposition method](https://en.wikipedia.org/wiki/Adomian_decomposition_method).

[^franklinTypo] Mind the typo in [#Franklin98] in the equation following
Eq. 4.53 on p. 105.

[^zohOrigin] The term 'zero-order' follows from the assumption that the
control input during a sampling period is constant. In other words,
$u(t)$ it is assumed to be a zeroth order function during that time
frame, since all of its derivatives are zero.

**Bibliography**:
[#Franklin98]: G. F. Franklin, M. L. Workman, and D. J. Powell,
_Digital control of dynamic systems_, 3rd ed. Half Moon Bay, CA:
Ellis-Kagle Press, 1998.

[#Al-Mohy12]: A. H. Al-Mohy and N. J. Higham, “Improved Inverse Scaling
and Squaring Algorithms for the Matrix Logarithm,” SIAM J. Sci. Comput.,
vol. 34, no. 4, pp. C153–C169, Jan. 2012, doi:
[10.1137/110852553](https://doi.org/10.1137/110852553). 

[#Higham08]: N. J. Higham, “Functions of Matrices.” Society for
Industrial and Applied Mathematics, Jan. 2008. doi:
[10.1137/1.9780898717778](https://doi.org/10.1137/1.9780898717778). 


<link rel="stylesheet" href="../latex.css?">
<!-- Markdeep: -->
<style class="fallback">
    body {
        visibility: hidden
    }
</style>
<script src="../markdeep.min.js?" charset="utf-8"></script>